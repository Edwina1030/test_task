# -*- coding: utf-8 -*-
"""test_task_playground.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12_XR87wI6jUf1A9VPqOHlEJkmni8EDsb
"""

# import relevante libraries
import torch 
import numpy as np
from torch import nn
from torchvision.datasets import mnist # get built-in dataset MNIST
from torch.utils.data import DataLoader # get iterative data
from torch.autograd import Variable # get variable
from torch import  optim
from torchvision import transforms
 
# use built-in function to download MNIST dataset 
train_set = mnist.MNIST('./data',
                        train=True, #creates datasets from training.pt
                        download=True
                        )
test_set = mnist.MNIST('./data',
                       train=False, #creates datasets from testing.pt
                       download=False
                       )
 
# Image Preprocessing in one
data_tf = transforms.Compose([transforms.ToTensor(), #transfer to Tensor type
                             transforms.Normalize([0.5],[0.5])]
                    # normalize tensor image with mean and standard deviation
                            )
 
train_set = mnist.MNIST('./data', # when local dataset is processed, change it
                        train=True,
                        transform=data_tf,
                        download=True
                        ) 
test_set = mnist.MNIST('./data', # when local dataset is processed, change it
                       train=False,
                       transform=data_tf,
                       download=True
                       )
# to get iterative data
train_data = DataLoader(train_set,
                        batch_size=64,
                        shuffle=True
                        )
test_data = DataLoader(test_set,
                       batch_size=128,
                       shuffle=False
                       )
 
# defnite model
class build_model(nn.Module):
    def __init__(self):
        super(build_model,self).__init__()
        
        self.layer1 = nn.Sequential(nn.Conv2d(in_channels = 1,
                                              out_channels = 16,
                                              kernel_size=3
                                              ), 
                                    nn.BatchNorm2d(16),
                                    nn.ReLU(inplace=True)
                                    )
        
        self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 16,
                                              out_channels = 32,
                                              kernel_size=3
                                              ),
                                    nn.BatchNorm2d(32),
                                    nn.ReLU(inplace=True),
                                    nn.MaxPool2d(kernel_size=2,stride=2)
                                    ) 
        
        self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 32,
                                              out_channels = 64,
                                              kernel_size=3
                                              ), 
                                    nn.BatchNorm2d(64),
                                    nn.ReLU(inplace=True)
                                    )
        
        self.layer4 = nn.Sequential(nn.Conv2d(64,128,kernel_size=3), 
                                    nn.BatchNorm2d(128),
                                    nn.ReLU(inplace=True),
                                    nn.MaxPool2d(kernel_size=2,stride=2)
                                    )  
        
        self.fc = nn.Sequential(nn.Linear(128 * 4 * 4,1024),
                                nn.ReLU(inplace=True),
                                nn.Linear(1024,128),
                                nn.ReLU(inplace=True),
                                nn.Linear(128,10)
                                )
        
    def forward(self,x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = x.view(x.size(0),-1)
        x = self.fc(x)
        
        return x

model = build_model()
print(model)

criterion = nn.CrossEntropyLoss() 
optimizer = optim.SGD(model.parameters(),lr = 0.001) # may change later

nums_epoch = 1 # for first try
 
# starting traning 
losses = []
acces = []
eval_losses = []
eval_acces = []
 
for epoch in range(nums_epoch):
    train_loss = 0
    train_acc = 0
    model = model.train()
    for img , label in train_data:
        img = Variable(img)
        label = Variable(label)
        
        # Forward propagation
        out = model(img)
        loss = criterion(out,label)
        
        # Back propagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Record loss
        train_loss += loss.item()
        
        # Calculate classification acc 
        _,pred = out.max(1)
        num_correct = (pred == label).sum().item()
        acc = num_correct / img.shape[0]
       
        train_acc += acc
        
    losses.append(train_loss / len(train_data))
    acces.append(train_acc / len(train_data))
    
    eval_loss = 0
    eval_acc = 0

    # testing daset will not be trained
    for img , label in test_data:
        #img = img.reshape(img.size(0),-1)
        img = Variable(img)
        label = Variable(label)
        
        out = model(img)
        
        loss = criterion(out,label)
        
         # Record loss
        eval_loss += loss.item()
        
        _ , pred = out.max(1)
        num_correct = (pred==label).sum().item()
        acc = num_correct / img.shape[0]
        
        eval_acc += acc
    eval_losses.append(eval_loss / len(test_data))
    eval_acces.append(eval_acc / len(test_data))
    
    print('Epoch {} Training Loss {} Training  Accuracy {} Testing Loss {} Testing Accuracy {}'.format(
        epoch+1, train_loss / len(train_data),train_acc / len(train_data), eval_loss / len(test_data), eval_acc / len(test_data)))